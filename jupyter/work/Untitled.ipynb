{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dac8422e-177d-4792-971a-0d7e291e7aab",
   "metadata": {},
   "source": [
    "# Sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c210396-3f99-4774-ace2-94f1719a88ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "757ec837-a4c2-4762-abf2-2eb9b1f1170b",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "    'Features': [\n",
    "        {\n",
    "            'FeatureName': 'target',\n",
    "            'FeatureType': 'float'\n",
    "        },\n",
    "        {\n",
    "            'FeatureName': 'group_id_0',\n",
    "            'FeatureType': 'int'\n",
    "        },\n",
    "        {\n",
    "            'FeatureName': 'group_id_1',\n",
    "            'FeatureType': 'int'\n",
    "        },\n",
    "        {\n",
    "            'FeatureName': 'timestamp',\n",
    "            'FeatureType': 'timestamp'\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87e8167b-cfeb-4075-ae54-4b2a11b71e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv')\n",
    "df = df.rename(\n",
    "    columns={\n",
    "        'store': 'group_id_0',\n",
    "        'item': 'group_id_1', \n",
    "        'date': 'timestamp', \n",
    "        'sales': 'target'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fba845e-9857-4551-b72d-c711cc119fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'] = df['target'].astype(float)\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db424edb-54c7-4dc9-8b14-37551db046a5",
   "metadata": {},
   "source": [
    "# Resolving bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5a4791f-fc73-4a3a-a359-c82c1848ced5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minio import Minio\n",
    "import pyarrow.parquet as pq\n",
    "import s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "73f5507a-eaaa-4f19-b214-78fb46b988d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetBridge:\n",
    "    \"\"\"Bridges buckets and dataset creation.\n",
    "    \"\"\"\n",
    "    MINIO_ENDPOINT = 'http://minio:9000'\n",
    "    ROOT_PATH = 'data/'\n",
    "\n",
    "    def __init__(self, bucket_name, access_key, secret_key):\n",
    "        self.bucket_name = bucket_name\n",
    "        self.access_key = access_key\n",
    "        self.secret_key = secret_key        \n",
    "\n",
    "    def get_parquet_dataset(self, base_dir):\n",
    "        s3_path = self._make_s3_root_path(base_dir)\n",
    "        fs = self._get_s3_filesystem()\n",
    "        parquet_dataset = pq.ParquetDataset(s3_path, filesystem=fs)\n",
    "        return parquet_dataset\n",
    "\n",
    "    def _make_s3_root_path(self, *args):\n",
    "        path = (\"s3://\" +\n",
    "                # self.ROOT_PATH +\n",
    "                self.bucket_name +\n",
    "                '/' +\n",
    "                '/'.join(args))\n",
    "        return path\n",
    "\n",
    "    def _get_s3_filesystem(self):\n",
    "        client_kwargs = {\n",
    "            'endpoint_url': self.MINIO_ENDPOINT,\n",
    "            'aws_access_key_id': self.access_key,\n",
    "            'aws_secret_access_key': self.secret_key,\n",
    "            'verify': False\n",
    "        }\n",
    "        fs = s3fs.S3FileSystem(anon=False, use_ssl=False,\n",
    "                               client_kwargs=client_kwargs)\n",
    "\n",
    "        return fs\n",
    "    \n",
    "    \n",
    "class Dataset:\n",
    "    \"\"\"Interface for parquet datasets.\n",
    "    \"\"\"\n",
    "    def __init__(self, parquet_ds):\n",
    "        self.parquet_ds = parquet_ds\n",
    "        \n",
    "    def get_pandas_df(self):\n",
    "        return self.parquet_ds.read_pandas().to_pandas()\n",
    "    \n",
    "    def get_group_ids(self):\n",
    "        arrow_schema = self.get_arrow_schema()\n",
    "        return [x for x in arrow_schema.names if x.startswith('group_id')]\n",
    "        \n",
    "    def get_arrow_schema(self):\n",
    "        return self.parquet_ds.schema.to_arrow_schema()\n",
    "    \n",
    "    def get_names(self):\n",
    "        arrow_schema = self.get_arrow_schema()\n",
    "        return arrow_schema.names\n",
    "    \n",
    "    def merge(self, parquet_ds):\n",
    "        left_df = self.get_pandas_df()\n",
    "        right_df = parquet_ds.get_pandas_df()\n",
    "        group_ids = self.get_group_ids()\n",
    "        merged_df = pd.merge(\n",
    "            left=left_df,\n",
    "            right=right_df,\n",
    "            on=group_ids + ['timestamp']\n",
    "        )\n",
    "        return merged_df\n",
    "    \n",
    "    \n",
    "class DatasetsCollector:\n",
    "    def __init__(self, **kwargs):\n",
    "        self._validate_datasets(**kwargs)\n",
    "        self.kwargs = kwargs\n",
    "        \n",
    "        \n",
    "    def get_dataset_by_name(self, name):\n",
    "        if name not in self.kwargs:\n",
    "            raise ValueError('Unkown dataset name {}'.format(name))\n",
    "        return self.kwargs[name]\n",
    "    \n",
    "    \n",
    "    def get_time_segmentation(self):\n",
    "        time_segmentation_resolver = TimeSegmentationResolver(**self.kwargs)\n",
    "        return time_segmentation_resolver.resolve()\n",
    "    \n",
    "    \n",
    "    def _get_datasets(self):\n",
    "        return list(self.kwargs.values())\n",
    "        \n",
    "        \n",
    "    def get_group_ids(self, validate=True):\n",
    "        datasets = self._get_datasets()\n",
    "        all_group_ids = [ds.get_group_ids() for ds in datasets]\n",
    "         \n",
    "        # Uniqueness for list of lists.\n",
    "        unique_group_ids = [list(x) for x in set(tuple(x) for x in u)]\n",
    "        \n",
    "        if validate:\n",
    "            if len(unique_group_ids) > 1:\n",
    "                raise\n",
    "            return unique_group_ids[0]\n",
    "        return unique_group_ids\n",
    "        \n",
    "        \n",
    "    def _validate_datasets(self, **kwargs):\n",
    "        for key, dataset in kwargs.items():\n",
    "            if not isinstance(dataset, Dataset):\n",
    "                raise TypeError(\n",
    "                    'All parameters must be of type Dataset. ' \n",
    "                    'Instead, kwarg {} received type {}'.format(key, type(dataset).__name__)\n",
    "                )\n",
    "                \n",
    "                \n",
    "class TimeSegmentationResolver:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.kwargs = kwargs\n",
    "        \n",
    "        \n",
    "    def resolve(self):\n",
    "        return {}\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6a3e4f40-da0b-4611-84e6-b2d1be5fc63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data bridge between minio and mlpi.\n",
    "bridge = DatasetBridge(bucket_name='sample', access_key='oxxo', secret_key='password')\n",
    "\n",
    "# Create ``dataset`` object which is a easy-to-use parquet dataset interface.\n",
    "parquet_dataset = bridge.get_parquet_dataset('target')\n",
    "dataset = Dataset(parquet_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0c129355-5606-43d6-a3cb-e67f9603f5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "collector = DatasetsCollector(target=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ec7677fd-c741-4a47-ae70-44042805207a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collector.get_time_segmentation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "df27dda8-fe55-4475-8973-13b13eda1e40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Dataset at 0xffff715c7e80>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collector.get_dataset_by_name('target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "47672711-abec-4c50-b3bb-faac6b8f4853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['group_id_0', 'group_id_1']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collector.get_group_ids(validate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459f178e-d7f8-4e59-a83e-f9d42fefbac1",
   "metadata": {},
   "source": [
    "# Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9057e6d6-61bf-40ca-a28a-e41d4d878e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ede2bb6-7431-42bf-bce3-95891923d174",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor(BaseModel):\n",
    "    name: str\n",
    "    algorithm: str\n",
    "    forecast_horizon = int\n",
    "    perform_hpo: bool = False\n",
    "    bucket_name: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8cec82-5d70-4c31-9486-46ee55a24b79",
   "metadata": {},
   "source": [
    "# Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa0f87af-7928-44a8-803d-d0fdb98005a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mooncake.nn import SeqToSeq, TemporalFusionTransformer as TFT\n",
    "\n",
    "ESTIMATORS = {\n",
    "    'seq2seq': SeqToSeq,\n",
    "    'tft': TFT\n",
    "}\n",
    "\n",
    "\n",
    "class EstimatorCreator:\n",
    "    def __init__(self, predictor, target_dataset):\n",
    "        self.predictor = predictor\n",
    "        self.target_dataset = target_dataset\n",
    "\n",
    "    def create_estimator(self):\n",
    "        cls = self._get_estimator_class()\n",
    "        estimator_args = self._get_estimator_args()\n",
    "        return cls(**estimator_args)\n",
    "\n",
    "    def _get_estimator_class(self):\n",
    "        return ESTIMATORS[self.predictor.algorithm]\n",
    "\n",
    "    def _get_estimator_args(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class EstimatorArgsCreator:\n",
    "    def __init__(self, predictor, target_dataset):\n",
    "        self.predictor = predictor\n",
    "        self.target_dataset = target_dataset\n",
    "\n",
    "    def get_estimator_args(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714d369e-397c-498b-bc85-2fa8ca34a3c4",
   "metadata": {},
   "source": [
    "# Putting it all together "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09a2e4be-475e-4bfb-89b0-4f91b8e64d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data bridge between minio and mlpi.\n",
    "bridge = DatasetBridge(bucket_name='sample', access_key='oxxo', secret_key='password')\n",
    "\n",
    "# Create ``dataset`` object which is a easy-to-use parquet dataset interface.\n",
    "parquet_dataset = bridge.get_parquet_dataset('target')\n",
    "dataset = Dataset(parquet_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8411db91-14b5-4b68-abf5-79e245d62b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictor object (request json)\n",
    "\n",
    "json_request = {\n",
    "    'name': 'sample',\n",
    "    'algorithm': 'seq2seq',\n",
    "    'forecast_horizon': 10,\n",
    "    'perform_hpo': False,\n",
    "    'bucket_name': 'sample'\n",
    "}\n",
    "\n",
    "predictor = Predictor(**json_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2180a979-b2ba-49d3-9e51-b83da790f4b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.EstimatorCreator at 0xffff2ced44c0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create estimator\n",
    "\n",
    "EstimatorCreator(predictor, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4110e803-ac4e-4d05-a4d1-f7f6d7657a19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['target', 'group_id_0', 'group_id_1', 'timestamp']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.get_names()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
