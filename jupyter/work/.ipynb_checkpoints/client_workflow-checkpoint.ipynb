{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dac8422e-177d-4792-971a-0d7e291e7aab",
   "metadata": {},
   "source": [
    "# Sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c210396-3f99-4774-ace2-94f1719a88ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "757ec837-a4c2-4762-abf2-2eb9b1f1170b",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "    'Features': [\n",
    "        {\n",
    "            'FeatureName': 'target',\n",
    "            'FeatureType': 'float'\n",
    "        },\n",
    "        {\n",
    "            'FeatureName': 'group_id_0',\n",
    "            'FeatureType': 'int'\n",
    "        },\n",
    "        {\n",
    "            'FeatureName': 'group_id_1',\n",
    "            'FeatureType': 'int'\n",
    "        },\n",
    "        {\n",
    "            'FeatureName': 'timestamp',\n",
    "            'FeatureType': 'timestamp'\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87e8167b-cfeb-4075-ae54-4b2a11b71e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/target.csv')\n",
    "df = df.rename(\n",
    "    columns={\n",
    "        'store': 'group_id_0',\n",
    "        'item': 'group_id_1', \n",
    "        'date': 'timestamp', \n",
    "        'sales': 'target'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fba845e-9857-4551-b72d-c711cc119fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'] = df['target'].astype(float)\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db424edb-54c7-4dc9-8b14-37551db046a5",
   "metadata": {},
   "source": [
    "# Resolving bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5a4791f-fc73-4a3a-a359-c82c1848ced5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minio import Minio\n",
    "import pyarrow.parquet as pq\n",
    "import s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "73f5507a-eaaa-4f19-b214-78fb46b988d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetBridge:\n",
    "    \"\"\"Bridges buckets and dataset creation.\n",
    "    \"\"\"\n",
    "    MINIO_ENDPOINT = 'http://minio:9000'\n",
    "    ROOT_PATH = 'data/'\n",
    "\n",
    "    def __init__(self, bucket_name, access_key, secret_key):\n",
    "        self.bucket_name = bucket_name\n",
    "        self.access_key = access_key\n",
    "        self.secret_key = secret_key        \n",
    "\n",
    "    def get_parquet_dataset(self, base_dir):\n",
    "        s3_path = self._make_s3_root_path(base_dir)\n",
    "        fs = self._get_s3_filesystem()\n",
    "        parquet_dataset = pq.ParquetDataset(s3_path, filesystem=fs)\n",
    "        return parquet_dataset\n",
    "\n",
    "    def _make_s3_root_path(self, *args):\n",
    "        path = (\"s3://\" +\n",
    "                # self.ROOT_PATH +\n",
    "                self.bucket_name +\n",
    "                '/' +\n",
    "                '/'.join(args))\n",
    "        return path\n",
    "\n",
    "    def _get_s3_filesystem(self):\n",
    "        client_kwargs = {\n",
    "            'endpoint_url': self.MINIO_ENDPOINT,\n",
    "            'aws_access_key_id': self.access_key,\n",
    "            'aws_secret_access_key': self.secret_key,\n",
    "            'verify': False\n",
    "        }\n",
    "        fs = s3fs.S3FileSystem(anon=False, use_ssl=False,\n",
    "                               client_kwargs=client_kwargs)\n",
    "\n",
    "        return fs\n",
    "    \n",
    "    \n",
    "class Dataset:\n",
    "    \"\"\"Interface for parquet datasets.\n",
    "    \"\"\"\n",
    "    def __init__(self, parquet_ds):\n",
    "        self.parquet_ds = parquet_ds\n",
    "        \n",
    "    def get_pandas_df(self):\n",
    "        return self.parquet_ds.read_pandas().to_pandas()\n",
    "    \n",
    "    def get_group_ids(self):\n",
    "        arrow_schema = self.get_arrow_schema()\n",
    "        return [x for x in arrow_schema.names if x.startswith('group_id')]\n",
    "        \n",
    "    def get_arrow_schema(self):\n",
    "        return self.parquet_ds.schema.to_arrow_schema()\n",
    "    \n",
    "    def get_names(self):\n",
    "        arrow_schema = self.get_arrow_schema()\n",
    "        return arrow_schema.names\n",
    "    \n",
    "    def merge(self, parquet_ds):\n",
    "        left_df = self.get_pandas_df()\n",
    "        right_df = parquet_ds.get_pandas_df()\n",
    "        group_ids = self.get_group_ids()\n",
    "        merged_df = pd.merge(\n",
    "            left=left_df,\n",
    "            right=right_df,\n",
    "            on=group_ids + ['timestamp']\n",
    "        )\n",
    "        return merged_df\n",
    "    \n",
    "    \n",
    "class DatasetsCollector:\n",
    "    \"\"\"Base class for dataset collectors.\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        self._validate_kwargs(**kwargs)\n",
    "        self.kwargs = kwargs\n",
    "        vars(self).update(self.kwargs)\n",
    "        \n",
    "        \n",
    "    def get_dataset_by_name(self, name):\n",
    "        if name not in self.kwargs:\n",
    "            raise ValueError('Unkown dataset name {}'.format(name))\n",
    "        return self.kwargs[name]\n",
    "        \n",
    "    def get_names(self, include_pk=True):\n",
    "        names = {\n",
    "            k: v.get_names() \n",
    "            if v is not None \n",
    "            else [] for k, v in self.kwargs.items()\n",
    "        }\n",
    "        if not include_pk:\n",
    "            group_ids = self.get_group_ids()\n",
    "            pk = group_ids + ['timestamp']\n",
    "            names_without_pk = {\n",
    "                k: [x for x in v if x not in pk] \n",
    "                for k, v in names.items()\n",
    "            }\n",
    "            return names_without_pk\n",
    "        return names\n",
    "        \n",
    "    def _get_datasets(self):\n",
    "        return list(self.kwargs.values())\n",
    "        \n",
    "        \n",
    "    def get_group_ids(self, validate=True):\n",
    "        datasets = self._get_datasets()\n",
    "        all_group_ids = [ds.get_group_ids() for ds in datasets if ds is not None]\n",
    "         \n",
    "        # Uniqueness for list of lists.\n",
    "        unique_group_ids = [list(x) for x in set(tuple(x) for x in all_group_ids)]\n",
    "        \n",
    "        if validate:\n",
    "            if len(unique_group_ids) > 1:\n",
    "                raise\n",
    "            return unique_group_ids[0]\n",
    "        return unique_group_ids\n",
    "        \n",
    "        \n",
    "    def _validate_kwargs(self, **kwargs):\n",
    "        for key, dataset in kwargs.items():\n",
    "            if dataset is not None:\n",
    "                if not isinstance(dataset, Dataset):\n",
    "                    raise TypeError(\n",
    "                        'All parameters must be of type Dataset. ' \n",
    "                        'Instead, kwarg {} received type {}'.format(key, type(dataset).__name__)\n",
    "                    )\n",
    "                    \n",
    "                    \n",
    "class TimeSeriesCollector(DatasetsCollector):\n",
    "    def __init__(\n",
    "        self, \n",
    "        target, \n",
    "        time_varying_known_reals=None, \n",
    "        time_varying_unknown_reals=None, \n",
    "        static_categoricals=None\n",
    "    ):\n",
    "        super().__init__(\n",
    "            target=target, \n",
    "            time_varying_known_reals=time_varying_known_reals, \n",
    "            time_varying_unknown_reals=time_varying_unknown_reals, \n",
    "            static_categoricals=static_categoricals\n",
    "        )\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "96401c27-4eec-4f62-b087-a25f8f411faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data bridge between minio and mlpi.\n",
    "bridge = DatasetBridge(bucket_name='sample', access_key='oxxo', secret_key='password')\n",
    "\n",
    "# Create ``dataset`` object which is a easy-to-use parquet dataset interface.\n",
    "parquet_dataset = bridge.get_parquet_dataset('target')\n",
    "dataset = Dataset(parquet_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0c129355-5606-43d6-a3cb-e67f9603f5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "collector = TimeSeriesCollector(\n",
    "    target=dataset, \n",
    "    time_varying_known_reals=None, \n",
    "    time_varying_unknown_reals=None, \n",
    "    static_categoricals=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e16dd34b-ccd8-4ee0-8792-1d6a339f73ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "collector.time_varying_unknown_reals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459f178e-d7f8-4e59-a83e-f9d42fefbac1",
   "metadata": {},
   "source": [
    "# Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9057e6d6-61bf-40ca-a28a-e41d4d878e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ede2bb6-7431-42bf-bce3-95891923d174",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor(BaseModel):\n",
    "    name: str\n",
    "    algorithm: str\n",
    "    forecast_horizon = int\n",
    "    perform_hpo: bool = False\n",
    "    bucket_name: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8cec82-5d70-4c31-9486-46ee55a24b79",
   "metadata": {},
   "source": [
    "# Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aa0f87af-7928-44a8-803d-d0fdb98005a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mooncake.nn import SeqToSeq, TemporalFusionTransformer as TFT\n",
    "from mooncake.helper import common_callbacks\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "\n",
    "class EstimatorCreator:\n",
    "    \n",
    "    ESTIMATORS = {\n",
    "    'seq2seq': SeqToSeq,\n",
    "    'tft': TFT\n",
    "    }\n",
    "    \n",
    "    def __init__(self, predictor, dataset_collector):\n",
    "        self.predictor = predictor\n",
    "        self.dataset_collector = dataset_collector\n",
    "\n",
    "    def create_estimator(self):\n",
    "        cls = self._get_estimator_class()\n",
    "        estimator_args = self._get_estimator_args()\n",
    "        return cls(**estimator_args)\n",
    "\n",
    "    def _get_estimator_class(self):\n",
    "        return self.ESTIMATORS[self.predictor.algorithm]\n",
    "\n",
    "    def _get_estimator_args(self):\n",
    "        args_creator = EstimatorArgsCreator(self.predictor, self.dataset_collector)\n",
    "        return args_creator.get_estimator_args()\n",
    "\n",
    "\n",
    "class EstimatorArgsCreator:\n",
    "    def __init__(self, predictor, dataset_collector):\n",
    "        self.predictor = predictor\n",
    "        self.dataset_collector = dataset_collector\n",
    "\n",
    "    def get_estimator_args(self):\n",
    "        time_segmentation = self.dataset_collector.get_names(include_pk=False)\n",
    "        group_ids = self.dataset_collector.get_group_ids()\n",
    "        max_prediction_length = self.predictor.forecast_horizon\n",
    "        time_idx = 'time_idx'\n",
    "        \n",
    "        # Callbacks.\n",
    "        lr_scheduler = dict(\n",
    "        policy=OneCycleLR, \n",
    "        step_every='batch', \n",
    "        max_lr=1e-3, \n",
    "        steps_per_epoch='iterations', \n",
    "        epochs='max_epochs'\n",
    "        )\n",
    "        callbacks = common_callbacks(lr_scheduler, gradient_clipping=True)\n",
    "            \n",
    "        return {\n",
    "            'group_ids': group_ids,\n",
    "            'max_prediction_length': max_prediction_length,\n",
    "            'time_idx': time_idx,\n",
    "            'max_encoder_length': 20,\n",
    "            'callbacks': callbacks,\n",
    "            **time_segmentation\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714d369e-397c-498b-bc85-2fa8ca34a3c4",
   "metadata": {},
   "source": [
    "# Putting it all together "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "09a2e4be-475e-4bfb-89b0-4f91b8e64d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data bridge between minio and mlpi.\n",
    "bridge = DatasetBridge(bucket_name='sample', access_key='oxxo', secret_key='password')\n",
    "\n",
    "# Create ``dataset`` object which is a easy-to-use parquet dataset interface.\n",
    "parquet_dataset = bridge.get_parquet_dataset('target')\n",
    "dataset = Dataset(parquet_dataset)\n",
    "\n",
    "# Collect all datasets in a `DatasetCollector` object\n",
    "dataset_collector = TimeSeriesCollector(\n",
    "    target=dataset, \n",
    "    time_varying_known_reals=None, \n",
    "    time_varying_unknown_reals=None, \n",
    "    static_categoricals=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8411db91-14b5-4b68-abf5-79e245d62b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictor object (request json)\n",
    "\n",
    "json_request = {\n",
    "    'name': 'sample',\n",
    "    'algorithm': 'seq2seq',\n",
    "    'forecast_horizon': 10,\n",
    "    'perform_hpo': False,\n",
    "    'bucket_name': 'sample',\n",
    "}\n",
    "\n",
    "predictor = Predictor(**json_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2180a979-b2ba-49d3-9e51-b83da790f4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create estimator\n",
    "\n",
    "estimator = EstimatorCreator(predictor, dataset_collector).create_estimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c132f60a-144b-48ad-b75e-921ddfd9d18f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
