{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dac8422e-177d-4792-971a-0d7e291e7aab",
   "metadata": {},
   "source": [
    "# Sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c210396-3f99-4774-ace2-94f1719a88ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "757ec837-a4c2-4762-abf2-2eb9b1f1170b",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "    'Features': [\n",
    "        {\n",
    "            'FeatureName': 'target',\n",
    "            'FeatureType': 'float'\n",
    "        },\n",
    "        {\n",
    "            'FeatureName': 'group_id_0',\n",
    "            'FeatureType': 'int'\n",
    "        },\n",
    "        {\n",
    "            'FeatureName': 'group_id_1',\n",
    "            'FeatureType': 'int'\n",
    "        },\n",
    "        {\n",
    "            'FeatureName': 'timestamp',\n",
    "            'FeatureType': 'timestamp'\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87e8167b-cfeb-4075-ae54-4b2a11b71e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv')\n",
    "df = df.rename(\n",
    "    columns={\n",
    "        'store': 'group_id_0',\n",
    "        'item': 'group_id_1', \n",
    "        'date': 'timestamp', \n",
    "        'sales': 'target'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fba845e-9857-4551-b72d-c711cc119fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'] = df['target'].astype(float)\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db424edb-54c7-4dc9-8b14-37551db046a5",
   "metadata": {},
   "source": [
    "# Resolving bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5a4791f-fc73-4a3a-a359-c82c1848ced5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minio import Minio\n",
    "import pyarrow.parquet as pq\n",
    "import s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73f5507a-eaaa-4f19-b214-78fb46b988d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetBridge:\n",
    "    \"\"\"Bridges buckets and dataset creation.\n",
    "    \"\"\"\n",
    "    MINIO_ENDPOINT = 'http://minio:9000'\n",
    "    ROOT_PATH = 'data/'\n",
    "\n",
    "    def __init__(self, bucket_name, access_key, secret_key):\n",
    "        self.bucket_name = bucket_name\n",
    "        self.access_key = access_key\n",
    "        self.secret_key = secret_key        \n",
    "\n",
    "    def get_parquet_dataset(self, base_dir):\n",
    "        s3_path = self._make_s3_root_path(base_dir)\n",
    "        fs = self._get_s3_filesystem()\n",
    "        parquet_dataset = pq.ParquetDataset(s3_path, filesystem=fs)\n",
    "        return parquet_dataset\n",
    "\n",
    "    def _make_s3_root_path(self, *args):\n",
    "        path = (\"s3://\" +\n",
    "                # self.ROOT_PATH +\n",
    "                self.bucket_name +\n",
    "                '/' +\n",
    "                '/'.join(args))\n",
    "        return path\n",
    "\n",
    "    def _get_s3_filesystem(self):\n",
    "        client_kwargs = {\n",
    "            'endpoint_url': self.MINIO_ENDPOINT,\n",
    "            'aws_access_key_id': self.access_key,\n",
    "            'aws_secret_access_key': self.secret_key,\n",
    "            'verify': False\n",
    "        }\n",
    "        fs = s3fs.S3FileSystem(anon=False, use_ssl=False,\n",
    "                               client_kwargs=client_kwargs)\n",
    "\n",
    "        return fs\n",
    "    \n",
    "    \n",
    "class Dataset:\n",
    "    \"\"\"Interface for parquet datasets.\n",
    "    \"\"\"\n",
    "    def __init__(self, parquet_ds):\n",
    "        self.parquet_ds = parquet_ds\n",
    "        \n",
    "    def get_pandas_df(self):\n",
    "        return self.parquet_ds.read_pandas().to_pandas()\n",
    "    \n",
    "    def get_group_ids(self):\n",
    "        arrow_schema = self.get_arrow_schema()\n",
    "        return [x for x in arrow_schema.names if x.startswith('group_id')]\n",
    "        \n",
    "    def get_arrow_schema(self):\n",
    "        return self.parquet_ds.schema.to_arrow_schema()\n",
    "    \n",
    "    def merge(self, parquet_ds):\n",
    "        left_df = self.get_pandas_df()\n",
    "        right_df = parquet_ds.get_pandas_df()\n",
    "        group_ids = self.get_group_ids()\n",
    "        merged_df = pd.merge(\n",
    "            left=left_df,\n",
    "            right=right_df,\n",
    "            on=group_ids + ['timestamp']\n",
    "        )\n",
    "        return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8cec82-5d70-4c31-9486-46ee55a24b79",
   "metadata": {},
   "source": [
    "# Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa0f87af-7928-44a8-803d-d0fdb98005a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mooncake.nn import SeqToSeq, TemporalFusionTransformer as TFT\n",
    "\n",
    "ESTIMATORS = {\n",
    "    'seq2seq': SeqToSeq,\n",
    "    'tft': TFT\n",
    "}\n",
    "\n",
    "\n",
    "class EstimatorCreator:\n",
    "    def __init__(self, predictor, target_dataset):\n",
    "        self.predictor = predictor\n",
    "        self.target_dataset = target_dataset\n",
    "\n",
    "    def create_estimator(self):\n",
    "        cls = self._get_estimator_class()\n",
    "        estimator_args = self._get_estimator_args()\n",
    "        return cls(**estimator_args)\n",
    "\n",
    "    def _get_estimator_class(self):\n",
    "        return ESTIMATORS[self.predictor.algorithm]\n",
    "\n",
    "    def _get_estimator_args(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class EstimatorArgsCreator:\n",
    "    def __init__(self, predictor, target_dataset):\n",
    "        self.predictor = predictor\n",
    "        self.target_dataset = target_dataset\n",
    "\n",
    "    def get_estimator_args(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714d369e-397c-498b-bc85-2fa8ca34a3c4",
   "metadata": {},
   "source": [
    "# Putting it all together "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09a2e4be-475e-4bfb-89b0-4f91b8e64d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data bridge between minio and mlpi.\n",
    "bridge = DatasetBridge(bucket_name='sample', access_key='oxxo', secret_key='password')\n",
    "\n",
    "# Create ``dataset`` object which is a easy-to-use parquet dataset interface.\n",
    "parquet_dataset = bridge.get_parquet_dataset('target')\n",
    "dataset = Dataset(parquet_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2180a979-b2ba-49d3-9e51-b83da790f4b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
