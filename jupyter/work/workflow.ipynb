{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dac8422e-177d-4792-971a-0d7e291e7aab",
   "metadata": {},
   "source": [
    "# Sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c210396-3f99-4774-ace2-94f1719a88ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "757ec837-a4c2-4762-abf2-2eb9b1f1170b",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "    'Features': [\n",
    "        {\n",
    "            'FeatureName': 'target',\n",
    "            'FeatureType': 'float'\n",
    "        },\n",
    "        {\n",
    "            'FeatureName': 'group_id_0',\n",
    "            'FeatureType': 'int'\n",
    "        },\n",
    "        {\n",
    "            'FeatureName': 'group_id_1',\n",
    "            'FeatureType': 'int'\n",
    "        },\n",
    "        {\n",
    "            'FeatureName': 'timestamp',\n",
    "            'FeatureType': 'timestamp'\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87e8167b-cfeb-4075-ae54-4b2a11b71e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/target.csv')\n",
    "df = df.rename(\n",
    "    columns={\n",
    "        'store': 'group_id_0',\n",
    "        'item': 'group_id_1', \n",
    "        'date': 'timestamp', \n",
    "        'sales': 'target'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fba845e-9857-4551-b72d-c711cc119fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'] = df['target'].astype(float)\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db424edb-54c7-4dc9-8b14-37551db046a5",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5a4791f-fc73-4a3a-a359-c82c1848ced5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import s3fs\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73f5507a-eaaa-4f19-b214-78fb46b988d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetBridge:\n",
    "    \"\"\"Bridges buckets and dataset creation.\n",
    "    \"\"\"\n",
    "    MINIO_ENDPOINT = 'http://minio:9000'\n",
    "    ROOT_PATH = 'data/'\n",
    "\n",
    "    def __init__(self, bucket_name, access_key, secret_key):\n",
    "        self.bucket_name = bucket_name\n",
    "        self.access_key = access_key\n",
    "        self.secret_key = secret_key        \n",
    "\n",
    "    def get_parquet_dataset(self, base_dir):\n",
    "        s3_path = self._make_s3_root_path(base_dir)\n",
    "        fs = self._get_s3_filesystem()\n",
    "        parquet_dataset = pq.ParquetDataset(s3_path, filesystem=fs)\n",
    "        return parquet_dataset\n",
    "\n",
    "    def _make_s3_root_path(self, *args):\n",
    "        path = (\"s3://\" +\n",
    "                # self.ROOT_PATH +\n",
    "                self.bucket_name +\n",
    "                '/' +\n",
    "                '/'.join(args))\n",
    "        return path\n",
    "\n",
    "    def _get_s3_filesystem(self):\n",
    "        client_kwargs = {\n",
    "            'endpoint_url': self.MINIO_ENDPOINT,\n",
    "            'aws_access_key_id': self.access_key,\n",
    "            'aws_secret_access_key': self.secret_key,\n",
    "            'verify': False\n",
    "        }\n",
    "        fs = s3fs.S3FileSystem(anon=False, use_ssl=False,\n",
    "                               client_kwargs=client_kwargs)\n",
    "\n",
    "        return fs\n",
    "    \n",
    "    \n",
    "class Dataset:\n",
    "    \"\"\"Interface for parquet datasets.\n",
    "    \"\"\"\n",
    "    def __init__(self, parquet_ds):\n",
    "        self.parquet_ds = parquet_ds\n",
    "        \n",
    "    def get_pandas_df(self):\n",
    "        return self.parquet_ds.read_pandas().to_pandas()\n",
    "    \n",
    "    def get_group_ids(self):\n",
    "        arrow_schema = self.get_arrow_schema()\n",
    "        return [x for x in arrow_schema.names if x.startswith('group_id')]\n",
    "        \n",
    "    def get_arrow_schema(self):\n",
    "        return self.parquet_ds.schema.to_arrow_schema()\n",
    "    \n",
    "    def get_names(self):\n",
    "        arrow_schema = self.get_arrow_schema()\n",
    "        return arrow_schema.names\n",
    "    \n",
    "    def merge(self, parquet_ds):\n",
    "        left_df = self.get_pandas_df()\n",
    "        right_df = parquet_ds.get_pandas_df()\n",
    "        group_ids = self.get_group_ids()\n",
    "        merged_df = pd.merge(\n",
    "            left=left_df,\n",
    "            right=right_df,\n",
    "            on=group_ids + ['timestamp']\n",
    "        )\n",
    "        return merged_df\n",
    "    \n",
    "    \n",
    "class DatasetsCollector:\n",
    "    \"\"\"Base class for dataset collectors.\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        self._validate_kwargs(**kwargs)\n",
    "        self.kwargs = kwargs\n",
    "        vars(self).update(self.kwargs)\n",
    "        \n",
    "        \n",
    "    def get_dataset_by_name(self, name):\n",
    "        if name not in self.kwargs:\n",
    "            raise ValueError('Unkown dataset name {}'.format(name))\n",
    "        return self.kwargs[name]\n",
    "        \n",
    "    def get_names(self, include_pk=True):\n",
    "        names = {\n",
    "            k: v.get_names() \n",
    "            if v is not None \n",
    "            else [] for k, v in self.kwargs.items()\n",
    "        }\n",
    "        if not include_pk:\n",
    "            group_ids = self.get_group_ids()\n",
    "            pk = group_ids + ['timestamp']\n",
    "            names_without_pk = {\n",
    "                k: [x for x in v if x not in pk] \n",
    "                for k, v in names.items()\n",
    "            }\n",
    "            return names_without_pk\n",
    "        return names\n",
    "        \n",
    "    def _get_datasets(self):\n",
    "        return list(self.kwargs.values())\n",
    "        \n",
    "        \n",
    "    def get_group_ids(self, validate=True):\n",
    "        datasets = self._get_datasets()\n",
    "        all_group_ids = [ds.get_group_ids() for ds in datasets if ds is not None]\n",
    "         \n",
    "        # Uniqueness for list of lists.\n",
    "        unique_group_ids = [list(x) for x in set(tuple(x) for x in all_group_ids)]\n",
    "        \n",
    "        if validate:\n",
    "            if len(unique_group_ids) > 1:\n",
    "                raise\n",
    "            return unique_group_ids[0]\n",
    "        return unique_group_ids\n",
    "    \n",
    "    def merge(self):\n",
    "        datasets = self._get_datasets()\n",
    "        dfs = [ds.get_pandas_df() for ds in datasets if ds is not None]\n",
    "        \n",
    "        pk = self.get_group_ids() + ['timestamp']\n",
    "        df_merge = reduce(lambda left, right: pd.merge(left, right, on=pk), dfs)\n",
    "        return df_merge\n",
    "        \n",
    "        \n",
    "        \n",
    "    def _validate_kwargs(self, **kwargs):\n",
    "        for key, dataset in kwargs.items():\n",
    "            if dataset is not None:\n",
    "                if not isinstance(dataset, Dataset):\n",
    "                    raise TypeError(\n",
    "                        'All parameters must be of type Dataset. ' \n",
    "                        'Instead, kwarg {} received type {}'.format(key, type(dataset).__name__)\n",
    "                    )\n",
    "                    \n",
    "                    \n",
    "class TimeSeriesCollector(DatasetsCollector):\n",
    "    def __init__(\n",
    "        self, \n",
    "        target, \n",
    "        time_varying_known_reals=None, \n",
    "        time_varying_unknown_reals=None, \n",
    "        static_categoricals=None\n",
    "    ):\n",
    "        super().__init__(\n",
    "            target=target, \n",
    "            time_varying_known_reals=time_varying_known_reals, \n",
    "            time_varying_unknown_reals=time_varying_unknown_reals, \n",
    "            static_categoricals=static_categoricals\n",
    "        )\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459f178e-d7f8-4e59-a83e-f9d42fefbac1",
   "metadata": {},
   "source": [
    "# Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9057e6d6-61bf-40ca-a28a-e41d4d878e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ede2bb6-7431-42bf-bce3-95891923d174",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor(BaseModel):\n",
    "    name: str\n",
    "    algorithm: str\n",
    "    forecast_horizon = int\n",
    "    perform_hpo: bool = False\n",
    "    bucket_name: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8cec82-5d70-4c31-9486-46ee55a24b79",
   "metadata": {},
   "source": [
    "# Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa0f87af-7928-44a8-803d-d0fdb98005a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mooncake.nn import SeqToSeq, TemporalFusionTransformer as TFT\n",
    "from mooncake.helper import common_callbacks\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "\n",
    "class EstimatorCreator:\n",
    "    \n",
    "    ESTIMATORS = {\n",
    "    'seq2seq': SeqToSeq,\n",
    "    'tft': TFT\n",
    "    }\n",
    "    \n",
    "    def __init__(self, predictor, dataset_collector):\n",
    "        self.predictor = predictor\n",
    "        self.dataset_collector = dataset_collector\n",
    "\n",
    "    def create_estimator(self):\n",
    "        cls = self._get_estimator_class()\n",
    "        estimator_args = self._get_estimator_args()\n",
    "        return cls(**estimator_args)\n",
    "\n",
    "    def _get_estimator_class(self):\n",
    "        return self.ESTIMATORS[self.predictor.algorithm]\n",
    "\n",
    "    def _get_estimator_args(self):\n",
    "        args_creator = EstimatorArgsCreator(self.predictor, self.dataset_collector)\n",
    "        return args_creator.get_estimator_args()\n",
    "\n",
    "\n",
    "class EstimatorArgsCreator:\n",
    "    def __init__(self, predictor, dataset_collector):\n",
    "        self.predictor = predictor\n",
    "        self.dataset_collector = dataset_collector\n",
    "\n",
    "    def get_estimator_args(self):\n",
    "        time_segmentation = self.dataset_collector.get_names(include_pk=False)\n",
    "        group_ids = self.dataset_collector.get_group_ids()\n",
    "        max_prediction_length = self.predictor.forecast_horizon\n",
    "        time_idx = 'time_idx'\n",
    "        \n",
    "        # Callbacks.\n",
    "        lr_scheduler = dict(\n",
    "        policy=OneCycleLR, \n",
    "        step_every='batch', \n",
    "        max_lr=1e-3, \n",
    "        steps_per_epoch='iterations', \n",
    "        epochs='max_epochs'\n",
    "        )\n",
    "        callbacks = common_callbacks(lr_scheduler, gradient_clipping=True)\n",
    "            \n",
    "        return {\n",
    "            'group_ids': group_ids,\n",
    "            'max_prediction_length': max_prediction_length,\n",
    "            'time_idx': time_idx,\n",
    "            'max_encoder_length': 20,\n",
    "            'callbacks': callbacks,\n",
    "            **time_segmentation\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da748d47-5991-4b99-b061-819706e6ba29",
   "metadata": {},
   "source": [
    "# Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ead8e5c3-c169-475d-928f-e370f914b480",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mooncake.preprocessing import (\n",
    "    GroupTransformer,\n",
    "    DataframeColumnTransformer,\n",
    "    CyclicalDates\n",
    ")\n",
    "from mooncake.helper import column_selector\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "\n",
    "\n",
    "class PreprocessorCreator:\n",
    "    def __init__(self, group_ids, scaler=MinMaxScaler(),\n",
    "                 encoder=OneHotEncoder(), cyclical_dates=True,\n",
    "                 time_idx='time_idx'):\n",
    "        self.group_ids = group_ids\n",
    "        self.scaler = scaler\n",
    "        self.encoder = encoder\n",
    "        self.cyclical_dates = cyclical_dates\n",
    "        self.time_idx = time_idx\n",
    "\n",
    "    def create_preprocessor(self):\n",
    "        group_trans = self._create_group_transformations()\n",
    "        outer_trans = self._create_outer_transformations()\n",
    "\n",
    "        steps = [\n",
    "            (\"groups\", GroupTransformer(group_trans, self.group_ids)),\n",
    "            (\"outer\", DataframeColumnTransformer(outer_trans))\n",
    "        ]\n",
    "        return Pipeline(steps=steps)\n",
    "\n",
    "    def _create_group_transformations(self):\n",
    "        \"\"\"Transformers defined here will act group by group.\n",
    "        That is, each group will be fitted to its own set of transformers.\n",
    "        \"\"\"\n",
    "        selector = column_selector(\n",
    "            dtype_include=['float', 'int'],\n",
    "            pattern_exclude=self.group_ids + [self.time_idx]\n",
    "        )\n",
    "        group_transformations = [\n",
    "            ('cont', self.scaler, selector),\n",
    "        ]\n",
    "        return group_transformations\n",
    "\n",
    "    def _create_outer_transformations(self):\n",
    "        timestamp_column = 'timestamp'\n",
    "        selector = column_selector(\n",
    "            dtype_include=['object'],\n",
    "            pattern_exclude=self.group_ids\n",
    "        )\n",
    "        outer_transformations = [\n",
    "            #('cat', self.encoder, selector),\n",
    "        ]\n",
    "        if self.cyclical_dates:\n",
    "            outer_transformations.append(\n",
    "                ('dates', CyclicalDates(), timestamp_column)\n",
    "            )\n",
    "        return outer_transformations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714d369e-397c-498b-bc85-2fa8ca34a3c4",
   "metadata": {},
   "source": [
    "# Putting it all together "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09a2e4be-475e-4bfb-89b0-4f91b8e64d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data bridge between minio and mlpi.\n",
    "bridge = DatasetBridge(bucket_name='sample', access_key='oxxo', secret_key='password')\n",
    "\n",
    "# Create ``dataset`` object which is a easy-to-use parquet dataset interface.\n",
    "parquet_dataset = bridge.get_parquet_dataset('target')\n",
    "dataset = Dataset(parquet_dataset)\n",
    "\n",
    "# Collect all datasets in a `DatasetCollector` object\n",
    "dataset_collector = TimeSeriesCollector(\n",
    "    target=dataset, \n",
    "    time_varying_known_reals=None, \n",
    "    time_varying_unknown_reals=None, \n",
    "    static_categoricals=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8411db91-14b5-4b68-abf5-79e245d62b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictor object (request json)\n",
    "\n",
    "json_request = {\n",
    "    'name': 'sample',\n",
    "    'algorithm': 'seq2seq',\n",
    "    'forecast_horizon': 10,\n",
    "    'perform_hpo': False,\n",
    "    'bucket_name': 'sample',\n",
    "}\n",
    "\n",
    "predictor = Predictor(**json_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2180a979-b2ba-49d3-9e51-b83da790f4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create estimator\n",
    "\n",
    "estimator = EstimatorCreator(predictor, dataset_collector).create_estimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c132f60a-144b-48ad-b75e-921ddfd9d18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess\n",
    "\n",
    "group_ids = dataset_collector.get_group_ids()\n",
    "X = dataset_collector.merge()\n",
    "X = X.astype({g: str for g in group_ids})\n",
    "group_ids = dataset_collector.get_group_ids()\n",
    "prepocessor_creator = PreprocessorCreator(group_ids)\n",
    "preprocessor = prepocessor_creator.create_preprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab44a348-f26e-4c27-b250-2f7f2c81cd64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day_sine</th>\n",
       "      <th>day_cos</th>\n",
       "      <th>month_sine</th>\n",
       "      <th>month_cos</th>\n",
       "      <th>target</th>\n",
       "      <th>group_id_0</th>\n",
       "      <th>group_id_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.201299</td>\n",
       "      <td>0.394356</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.105960</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.571268</td>\n",
       "      <td>0.724793</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.178808</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.848644</td>\n",
       "      <td>0.937752</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.039735</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.988468</td>\n",
       "      <td>0.998717</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.158940</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.968077</td>\n",
       "      <td>0.897805</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.165563</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91295</th>\n",
       "      <td>-0.250653</td>\n",
       "      <td>-0.050649</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.220238</td>\n",
       "      <td>9</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91296</th>\n",
       "      <td>0.151428</td>\n",
       "      <td>0.347305</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.303571</td>\n",
       "      <td>9</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91297</th>\n",
       "      <td>0.528964</td>\n",
       "      <td>0.688967</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.404762</td>\n",
       "      <td>9</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91298</th>\n",
       "      <td>0.820763</td>\n",
       "      <td>0.918958</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>9</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91299</th>\n",
       "      <td>0.979530</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.398810</td>\n",
       "      <td>9</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91300 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       day_sine   day_cos  month_sine  month_cos    target group_id_0  \\\n",
       "0      0.201299  0.394356         0.5        0.5  0.105960         10   \n",
       "1      0.571268  0.724793         0.5        0.5  0.178808         10   \n",
       "2      0.848644  0.937752         0.5        0.5  0.039735         10   \n",
       "3      0.988468  0.998717         0.5        0.5  0.158940         10   \n",
       "4      0.968077  0.897805         0.5        0.5  0.165563         10   \n",
       "...         ...       ...         ...        ...       ...        ...   \n",
       "91295 -0.250653 -0.050649         1.0        1.0  0.220238          9   \n",
       "91296  0.151428  0.347305         1.0        1.0  0.303571          9   \n",
       "91297  0.528964  0.688967         1.0        1.0  0.404762          9   \n",
       "91298  0.820763  0.918958         1.0        1.0  0.380952          9   \n",
       "91299  0.979530  1.000000         1.0        1.0  0.398810          9   \n",
       "\n",
       "      group_id_1  \n",
       "0             13  \n",
       "1             13  \n",
       "2             13  \n",
       "3             13  \n",
       "4             13  \n",
       "...          ...  \n",
       "91295         28  \n",
       "91296         28  \n",
       "91297         28  \n",
       "91298         28  \n",
       "91299         28  \n",
       "\n",
       "[91300 rows x 7 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66872d2-5622-4a38-9bc3-83ec65578d07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
