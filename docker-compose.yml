version: "3"
services:
  
  rabbitmq:
    # Message broker for Celery.
    restart: always
    build: ./rabbitmq
    image: rabbitmq
    container_name: rabbitmq
    expose:
      - ${RABBITMQ_PORT}
    ports:
      - ${RABBITMQ_PORT}:${RABBITMQ_PORT}
    networks:
      - backend
      - hadoop-spark-docker_hadoop_net
    volumes:
      - rabbitmq_data:/var/lib/rabbitmq/data

  client:
    # `client` container contains a 'client' directory which aims 
    # to be an installable python library that will serve as a http 
    # client for the `api` service.
    restart: always
    build: ./client
    image: client
    container_name: client
    environment:
      - AWS_ACCESS_KEY_ID=${MINIO_ACCESS_KEY}
      - AWS_SECRET_ACCESS_KEY=${MINIO_SECRET_KEY}
      - MLFLOW_S3_ENDPOINT_URL=${MINIO_URL}
      - MLFLOW_TRACKING_URI=http://mlflow:${MLFLOW_PORT}
    volumes:
    - ./client:/home/worker/client
    networks:
      - frontend
      - backend
      - hadoop-spark-docker_hadoop_net
    stdin_open: true # docker run -i
    tty: true        # docker run -t
    command:
      # Keep a main bash session always active.
      - bash

  api:
    # Container with the purpose of offering machine learning as a service.
    # This `api` service uses FastAPI for handling http requests and 
    # Celery for task queues.
    restart: always
    build: ./api
    image: api
    container_name: api
    environment:
      - AWS_ACCESS_KEY_ID=${MINIO_ACCESS_KEY}
      - AWS_SECRET_ACCESS_KEY=${MINIO_SECRET_KEY}
      - MLFLOW_S3_ENDPOINT_URL=${MINIO_URL}
      - MLFLOW_TRACKING_URI=http://mlflow:${MLFLOW_PORT}
    expose:
      - ${API_PORT}
    ports:
      - ${API_PORT}:${API_PORT} 
    volumes:
      - ./api:/home/worker/api
    networks:
      - frontend
      - backend
      - hadoop-spark-docker_hadoop_net
    # Keep a main bash session always active.
    stdin_open: true # docker run -i
    tty: true        # docker run -t
    command:
      - bash
  
  jupyter:
    # `jupyter` container serves jupyterlab services for experimental purposes.
    restart: always
    build: ./jupyter
    image: jupyter
    container_name: jupyter
    environment:
      - AWS_ACCESS_KEY_ID=${MINIO_ACCESS_KEY}
      - AWS_SECRET_ACCESS_KEY=${MINIO_SECRET_KEY}
      - MLFLOW_S3_ENDPOINT_URL=${MINIO_URL}
      - MLFLOW_TRACKING_URI=http://mlflow:${MLFLOW_PORT}
    networks:
      - frontend
      - backend
      - hadoop-spark-docker_hadoop_net
    ports:
      # It is sufficient to map port <JUPYTER_PORT> outside to 8888 inside the container.
      # Note: The displayed connection URL will be incorrect, youâ€™ll need to replace 8888 with <JUPYTER_PORT>, 
      # that is, http://127.0.0.1:<JUPYTER_PORT>/?token=TOKEN should work.
      - ${JUPYTER_PORT}:8888
    volumes:
      - ./jupyter/work:/home/jovyan/work
      - ./client:/home/jovyan/client
      - ./api:/home/jovyan/api
    depends_on:
      - mlflow
      
  postgres:
    restart: always
    build: ./postgres
    image: postgres
    container_name: postgres
    expose:
      - ${POSTGRES_PORT}
    ports:
      - 6543:${POSTGRES_PORT}
    networks:
      - backend
      - hadoop-spark-docker_hadoop_net
    environment:
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
    volumes:
      - pg_data:/var/lib/postgresql/data

  minio:
    restart: always
    image: minio/minio
    container_name: minio
    working_dir: /minio/storage
    expose:
      - ${MINIO_API_PORT}
    ports:
      - ${MINIO_API_PORT}:${MINIO_API_PORT}
      - ${MINIO_CONSOLE_PORT}:${MINIO_CONSOLE_PORT}
    networks:
      - frontend
      - backend
      - hadoop-spark-docker_hadoop_net
    environment:
      # Both keys are the same as aws keys (see mlflow environmet)
      - MINIO_ROOT_USER=$MINIO_ACCESS_KEY
      - MINIO_ROOT_PASSWORD=$MINIO_SECRET_KEY
    volumes:
      - minio_data:/data
    command: server /minio/storage --console-address :${MINIO_CONSOLE_PORT}

  mlflow:
    build: ./mlflow
    image: mlflow
    container_name: mlflow
    expose:
      - ${MLFLOW_PORT}
    ports:
      - ${MLFLOW_PORT}:${MLFLOW_PORT}
    networks:
      - frontend
      - backend
      - hadoop-spark-docker_hadoop_net
    environment:
      - BACKEND=postgresql://postgres:${POSTGRES_PASSWORD}@postgres:${POSTGRES_PORT}/mlflow
      # For artifact store in local: (note mlrun_data volume must also be uncommented)
      # - ARTIFACTS=/mlruns
      # Otherwise, for artifact store in AWS S3: (note boto was installed in container)
      - MLFLOW_S3_ENDPOINT_URL=${MINIO_URL}
      - AWS_ACCESS_KEY_ID=${MINIO_ACCESS_KEY}
      - AWS_SECRET_ACCESS_KEY=${MINIO_SECRET_KEY}      
    command:
      - sh  # (sh allows for var substitution of BACKEND and ARTIFACTS)
      - -c
      - mlflow server 
          --host 0.0.0.0
          --port ${MLFLOW_PORT}
          --backend-store-uri $${BACKEND} 
          --default-artifact-root ${ARTIFACT_PATH}
    depends_on:
      - postgres
      - minio
    # volumes:
    #   - mlrun_data:/mlruns

networks:
  frontend:
    driver: bridge
  backend:
    driver: bridge
  hadoop-spark-docker_hadoop_net:
    external: true

# When you use a named volume like "data:/container/path", you must declare it at the docker-compose file.
# This is not necessary when volumes are host paths.
volumes:
  pg_data:
  minio_data:
  rabbitmq_data:

